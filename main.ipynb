{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2SuxUZDCP6H"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit transformers SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7Z3wWLsCKUC",
        "outputId": "3fe5548f-ec3c-4daf-9530-1019aa6adbaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "# Import dependancies\n",
        "import streamlit as st\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "from PIL import Image\n",
        "from zmq import device\n",
        "import torch\n",
        "\n",
        "# Streamlit configuration\n",
        "st.set_page_config(page_title='Muhammed Summary')\n",
        "hide_menu_style = \"\"\"\n",
        "    <style>\n",
        "    @MainMenu {visibility: hidden; }\n",
        "    footer {visibility: hidden;}\n",
        "    </style>\n",
        "    \"\"\"\n",
        "st.markdown(hide_menu_style, unsafe_allow_html=True)\n",
        "\n",
        "# Define Functions\n",
        "def load_selected_tokenizer(tname):\n",
        "    tokenizer = PegasusTokenizer.from_pretrained(tname)\n",
        "    return tokenizer\n",
        "\n",
        "def load_selected_model(mname):\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(mname).to(device)\n",
        "    return model\n",
        "\n",
        "# Defining main Function\n",
        "def pegasus_summarize(text):\n",
        "    batch = tok(text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "    # Hyperparameter Tuning\n",
        "    gen = model.generate(\n",
        "        **batch,\n",
        "        max_length = max_length, # maximum length of sequence(summary)\n",
        "        min_length = min_length, # minimum length of sequences(summary)\n",
        "        do_sample = False, # Whether or not to use sampling ; use greedy decoding otherwise.\n",
        "        temperature = 1.0, # value used to module the next token probabilities\n",
        "        top_k =50, # The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "        top_p=1.00, # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
        "        repetition_penalty = rep_penalty, # The parameter for repetition penalty. 1.0 means no penalty\n",
        "        length_penalty = len_penalty, #1, # if more than 1 encourage model to generate #larger sequences\n",
        "        num_return_sequences=gens ) # Number of generated sequences(summeries) to output\n",
        "    # Decoding Summary\n",
        "    summary = tok.batch_decode(gen, skip_special_tokens=True)\n",
        "    return (summary)\n",
        "\n",
        "# Give a title to our app\n",
        "st.title('Paraphrasive Abstractive Summary (using default parameters)')\n",
        "\n",
        "# Take sequence length range from user\n",
        "dev = st.radio(\"Choose proccesing device (Auto by defualt)\", options=('auto','gpu','cpu'))\n",
        "if(dev == 'auto'):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "elif (dev == 'gpu'):\n",
        "    device ='cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "max_length = st.number_input(\"Max Sequence (Default selected)\", value = 64.0)\n",
        "min_length = st.number_input(\"Min Sequence (Default selected)\", value = 10)\n",
        "len_penalty= st.number_input(\"Length Penalty (Default selected)\", value = 0.6)\n",
        "rep_penalty=st.number_input(\"Repetition Penalty (Default selected)\", value = 1.0)\n",
        "gens=st.number_input(\"How many summaries to generate?\", value=1)\n",
        "\n",
        "status = st.radio('Select your pretrained model', options=('xsum','large'))\n",
        "\n",
        "# Check for which choice and act accordingly\n",
        "if(status == 'large'):\n",
        "\tmname = \"google/pegasus-large\"\n",
        "\ttname = \"google/pegasus-large\" # here we could use mname for both but for simplicity we use seperate names\n",
        "\n",
        "    ### scrapped try and except until more models are tested\n",
        "\t# try:\n",
        "\t# \tPegasusTokenizerFast.from_pretrained(\"google/pegasus-large\")\n",
        "\t# except:\n",
        "\t# \tst.text(\"Enter a valid model\")\n",
        "\t\t###\n",
        "elif(status == 'xsum'):\n",
        "\tmname = \"google/pegasus-xsum\"\n",
        "\ttname = \"google/pegasus-xsum\"\n",
        "\n",
        "    ### scrapped try and except until more models are tested\n",
        "\t# try:\n",
        "\t# \tPegasusTokenizerFast.from_pretrained(\"google/pegasus-xsum\")\n",
        "\t# except:\n",
        "\t# \tst.text(\"Enter a valid model\")\n",
        "    ###\n",
        "\n",
        "\n",
        "### Declaring Global Variables\n",
        "\n",
        "# Original/source text to be summerized\n",
        "src_text = st.text_input(\"Source Text\", placeholder=\"Two roads diverged in a wood, and Iâ€” I took the one less traveled by, And that has made all the difference.\")\n",
        "# Shows selected r\n",
        "# Loading the model and tokenizer\n",
        "tok = load_selected_tokenizer(mname)\n",
        "model = load_selected_model(mname)\n",
        "###\n",
        "\n",
        "# Generate the Summary!\n",
        "if(st.button('Generate Summary')):\n",
        "    result = pegasus_summarize(src_text)\n",
        "    st.success(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCWRqVFMCuaY",
        "outputId": "fe26f621-6991-49ab-b740-7704593d44b7"
      },
      "outputs": [],
      "source": [
        "! streamlit run main.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Abstrat.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
